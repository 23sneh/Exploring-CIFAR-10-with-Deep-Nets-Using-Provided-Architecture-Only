{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Read dataset and create dataloaders\n",
    "\n",
    "# Define transformations for the training set\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),      # Randomly crop the image to 32x32 and pad with 4 pixels on each side\n",
    "    transforms.RandomHorizontalFlip(),         # Randomly flip the image horizontally\n",
    "    transforms.ToTensor(),                      # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))   # Normalize the tensor with mean and standard deviation\n",
    "])\n",
    "\n",
    "# Define transformations for the test set\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),                      # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))   # Normalize the tensor with mean and standard deviation\n",
    "])\n",
    "\n",
    "# Load the CIFAR10 training set and apply the defined transformations\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
    "\n",
    "# Create a dataloader for the training set with a batch size of 128\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load the CIFAR10 test set and apply the defined transformations\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n",
    "\n",
    "# Create a dataloader for the test set with a batch size of 128\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k):\n",
    "        super(Block, self).__init__()\n",
    "        # create a linear layer to generate attention coefficients\n",
    "        self.linear = nn.Linear(in_channels, out_channels*k)\n",
    "        # create a list of convolutional layers\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels, out_channels, 3, padding=1) for _ in range(k)])\n",
    "        # create a ReLU activation function\n",
    "        self.activation = nn.ReLU()\n",
    "        # create an adaptive average pooling layer to generate a feature vector of size (batch_size, in_channels)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass the input tensor through the adaptive average pooling layer and squeeze the resulting tensor\n",
    "        a = self.activation(self.linear(self.avgpool(x).squeeze()))\n",
    "        # pass the feature vector through the linear layer and apply the ReLU activation function\n",
    "        k = len(self.convs)\n",
    "        # split the attention coefficients into k chunks and apply each chunk to one of the convolutional layers\n",
    "        o = sum(ai.unsqueeze(2).unsqueeze(3) * conv(x) for ai, conv in zip(a.chunk(k, 1), self.convs))\n",
    "        return o\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_blocks=3, in_channels=3, out_channels=10, block_out_channels=[256, 1024, 4096], block_k=4, dropout_rate=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        # Number of blocks in the network\n",
    "        self.num_blocks = num_blocks\n",
    "        # Number of input channels for the first block\n",
    "        self.in_channels = in_channels\n",
    "        # Number of output classes\n",
    "        self.out_channels = out_channels\n",
    "        # List of output channels for each block\n",
    "        self.block_out_channels = block_out_channels\n",
    "        # Value of k for each block\n",
    "        self.block_k = block_k\n",
    "        # Dropout rate for the classifier\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Define the blocks of the network\n",
    "        self.blocks = nn.ModuleList([Block(self.in_channels if i==0 else self.block_out_channels[i-1], self.block_out_channels[i], self.block_k) for i in range(num_blocks)])\n",
    "        # Define batch normalization layers for each block\n",
    "        self.bn = nn.ModuleList([nn.BatchNorm2d(self.block_out_channels[i]) for i in range(num_blocks)])\n",
    "        # Define ReLU activation layers for each block\n",
    "        self.relu = nn.ModuleList([nn.ReLU() for _ in range(num_blocks)])\n",
    "        # Define pooling layers for each block\n",
    "        self.pool = nn.ModuleList([nn.MaxPool2d(2, 2) for _ in range(num_blocks)])\n",
    "        # Define the classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.block_out_channels[-1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(256, self.out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each block in the network\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.blocks[i](x)\n",
    "            x = self.relu[i](x)\n",
    "            x = self.pool[i](x)\n",
    "            x = self.bn[i](x)\n",
    "        # Apply adaptive average pooling to the output of the last block\n",
    "        x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "        # Flatten the output of the last block\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Apply the classifier layers\n",
    "        out = self.classifier(x)\n",
    "        return out\n",
    "\n",
    "#Define the model, loss function, and optimizer\n",
    "model = Net()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, optimizer, device):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    # Move the model to the specified device\n",
    "    net.to(device)\n",
    "    # Initialize lists to store the loss and accuracy during training and testing\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    # Iterate over the specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0.0\n",
    "        train_total = 0.0\n",
    "        # Iterate over the training data loader\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "          # Set the model to training mode and reset the optimizer gradients\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Move the data and labels to the specified device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass to get the predicted labels and compute the loss\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            # Backward pass to compute the gradients and update the model parameters\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            # Compute the training loss and accuracy\n",
    "            with torch.no_grad():\n",
    "                train_loss += l.item()\n",
    "                train_correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "                train_total += y.size(0)\n",
    "        train_loss /= (i + 1)\n",
    "        train_acc = train_correct / train_total\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        # Compute the test accuracy\n",
    "        test_acc = evaluate_accuracy(net, test_iter, device)\n",
    "        # Append the loss and accuracy values to the corresponding lists\n",
    "        test_acc_list.append(test_acc)\n",
    "        # Print the epoch number and the loss and accuracy values\n",
    "        print(f\"epoch {epoch + 1}, train loss {train_loss:.3f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}\")\n",
    "    return train_loss_list, train_acc_list, test_acc_list\n",
    "\n",
    "def evaluate_accuracy(net, data_iter, device=None):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    # Use the device of the model if not specified\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()  \n",
    "    correct, total = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "      # Iterate over the data loader\n",
    "        for X, y in data_iter:\n",
    "          # Move the data and labels to the specified device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass to get the predicted labels and compute the accuracy\n",
    "            correct += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "            total += y.size(0)\n",
    "            # Return the accuracy value\n",
    "    return correct / total\n",
    "    \n",
    "num_epochs = 20\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if torch.cuda.is_available(): print(torch.cuda.get_device_name(0))\n",
    "\n",
    "train(model, trainloader, testloader, loss, num_epochs, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model and obtain the loss and accuracy lists\n",
    "train_loss_list, train_acc_list, test_acc_list = train(model, trainloader, testloader, loss, num_epochs, optimizer, device)\n",
    "#Create a plot of the training loss and accuracy against the number of epochs\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.plot(range(1, num_epochs + 1), train_loss_list, color=color, label='train loss')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.plot(range(1, num_epochs + 1), train_acc_list, color=color, label='train acc')\n",
    "ax2.plot(range(1, num_epochs + 1), test_acc_list, color='purple', label='test acc')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Evaluate the final testing accuracy of the model\n",
    "final_test_acc = evaluate_accuracy(model, testloader, device)\n",
    "print(f\"Final testing accuracy: {final_test_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
